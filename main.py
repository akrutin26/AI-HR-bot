import os
import io
import json
import asyncio
import re
import uuid
import base64
import numpy as np
import torch
import soundfile as sf
import resampy
from datetime import datetime
from pathlib import Path
from scipy.io.wavfile import write as write_wav
from pydub import AudioSegment
from PyPDF2 import PdfReader
from docx import Document
import pdfplumber  # –î–ª—è –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è PDF
from fastapi import File, UploadFile, Form
from typing import Optional
import re  # –î–æ–±–∞–≤—å—Ç–µ –∏–º–ø–æ—Ä—Ç –≤ –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –Ω–µ
import librosa

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è FastAPI
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict

# –í–∞—à–∏ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è AI-–º–æ–¥–µ–ª–µ–π
from faster_whisper import WhisperModel
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from TTS.tts.configs.xtts_config import XttsConfig
from TTS.tts.models.xtts import Xtts
from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# =======================
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
# =======================
app = FastAPI()
REPORTS_DIR = Path("reports")
REPORTS_DIR.mkdir(exist_ok=True)  # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
INTERVIEW_DATA_FILE = "interview_data.json"
chat_history: Dict[str, list] = {}  # –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å–µ—Å—Å–∏–∏ –ø–æ interview_id

# =======================
# Pydantic –º–æ–¥–µ–ª–∏
# =======================
class VacancyItem(BaseModel):
    key: str = Field(..., pattern=r"^[a-zA-Z0-9_-]+$", description="–ö–ª—é—á: —Ç–æ–ª—å–∫–æ –ª–∞—Ç–∏–Ω–∏—Ü–∞, —Ü–∏—Ñ—Ä—ã, _ –∏ -")
    title: str
    requirements: str
    questions: List[str]
    script: str
    criteria_weights: str = Field("{}", description="JSON-—Å—Ç—Ä–æ–∫–∞ —Å –≤–µ—Å–∞–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä: {\"technical_skills\": 50, \"communication\": 30, \"cases\": 20}")

class LLMAnalysisResponse(BaseModel):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM, –≤–∫–ª—é—á–∞—é—â–∞—è –ø—É–±–ª–∏—á–Ω—ã–π –æ—Ç–≤–µ—Ç –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –∞–Ω–∞–ª–∏–∑."""
    assistant_response: str
    internal_analysis: str

# =======================
# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ —Ä–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏
# =======================
def read_data() -> dict:
    """–ß–∏—Ç–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞."""
    if not os.path.exists(INTERVIEW_DATA_FILE):
        return {}
    try:
        with open(INTERVIEW_DATA_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–ª–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å {INTERVIEW_DATA_FILE}. –í–æ–∑–≤—Ä–∞—â–µ–Ω –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å.")
        return {}

def write_data(data: dict):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–∞–π–ª."""
    with open(INTERVIEW_DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# =======================
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
# =======================
print("–ó–∞–≥—Ä—É–∑–∫–∞ AI –º–æ–¥–µ–ª–µ–π...")

# 1. TTS Model
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ TTS (XTTS v2)...")
tts_config = XttsConfig()
tts_config.load_json("../tts/tts_models--multilingual--multi-dataset--xtts_v2/config.json")
tts_model = Xtts.init_from_config(tts_config)
tts_model.load_checkpoint(tts_config, checkpoint_dir="../tts/tts_models--multilingual--multi-dataset--xtts_v2/", use_deepspeed=True)
tts_model.cuda()
speaker_name = "Luis Moray"
gpt_cond_latent, speaker_embedding = tts_model.speaker_manager.speakers[speaker_name].values()

# 2. LLM Model
print("–ó–∞–≥—Ä—É–∑–∫–∞ LLM (OpenAI)...")
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, openai_api_base="https://api.proxyapi.ru/openai/v1", model_name="gpt-4o-mini", temperature=0.7)

prompt_template = PromptTemplate(
    input_variables=["chat_history", "emotion", "pauses_info", "structure_analysis", "vacancy", "requirements", "questions_list", "interview_script", "criteria_weights"],
    template="""–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –ø—Ä–æ–≤–æ–¥—è—â–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é {vacancy}.
–¢–≤–æ–π —Å—Ç–∏–ª—å –æ–±—â–µ–Ω–∏—è: –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, –Ω–æ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π. –ò–∑–±–µ–≥–∞–π –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤.
–¢–≤–æ—è –≥–ª–∞–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ ‚Äî –≥–∏–±–∫–æ —É–ø—Ä–∞–≤–ª—è—Ç—å —Ö–æ–¥–æ–º –±–µ—Å–µ–¥—ã, –∞ –Ω–µ —Å–ª–µ–ø–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å —Å–ø–∏—Å–∫—É –≤–æ–ø—Ä–æ—Å–æ–≤.

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç—É:**
{requirements}

**–í–µ—Å–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–π –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è):**
{criteria_weights}

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**
1. **–ò—Å–ø–æ–ª—å–∑—É–π —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:** –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞–Ω–¥–∏–¥–∞—Ç—É. –¢–≤–æ–∏ –≤–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –ø–æ–º–æ–≥–∞—Ç—å —Ä–∞—Å–∫—Ä—ã—Ç—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç —ç—Ç–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º.
2. **–°–ª–µ–¥—É–π —Å–∫—Ä–∏–ø—Ç—É:** –£ —Ç–µ–±—è –µ—Å—Ç—å –æ–±—â–∏–π –ø–ª–∞–Ω –≤–µ–¥–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞: {interview_script}.
3. **–ò—Å–ø–æ–ª—å–∑—É–π –≤–æ–ø—Ä–æ—Å—ã –∫–∞–∫ –æ—Ä–∏–µ–Ω—Ç–∏—Ä:** –í–æ—Ç –ø—Ä–∏–º–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è: {questions_list}. –ù–µ –∑–∞–¥–∞–≤–∞–π –∏—Ö –ø–æ–¥—Ä—è–¥. –í—ã–±–∏—Ä–∞–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç–≤–µ—Ç–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –≤–∞–∫–∞–Ω—Å–∏–∏.
4. **–ê–¥–∞–ø—Ç–∏—Ä—É–π—Å—è:** –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –≥–ª—É–±–æ–∫–∏–π –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π, –∑–∞–¥–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —ç—Ç–æ–π –∂–µ —Ç–µ–º–µ. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–π, –º–æ–∂–µ—à—å –∑–∞–¥–∞—Ç—å –Ω–∞–≤–æ–¥—è—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø–ª–∞–≤–Ω–æ –ø–µ—Ä–µ–π—Ç–∏ –∫ –¥—Ä—É–≥–æ–π —Ç–µ–º–µ.
5. **–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π soft skills:** –û–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –æ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:
    - –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞: {emotion}.
    - –ê–Ω–∞–ª–∏–∑ –ø–∞—É–∑ –≤ —Ä–µ—á–∏: {pauses_info}. –ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–¥–ª–∏–Ω–Ω—ã–µ –ø–∞—É–∑—ã ‚Äî –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –∫–æ—Ä–æ—Ç–∫–∏–µ ‚Äî —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å).
    - –õ–æ–≥–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞: {structure_analysis}.
    –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –∑–∞–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ) –∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å. –ù–µ –Ω—É–∂–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ —Å–∏–≥–Ω–∞–ª—ã –≤—Å–ª—É—Ö.
6. **–í–µ–¥–∏ –¥–∏–∞–ª–æ–≥ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ:** –ù–µ –ø—Ä–µ–≤—Ä–∞—â–∞–π —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –¥–æ–ø—Ä–æ—Å. –¢–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ª–æ–≥–∏—á–Ω—ã–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º –¥–∏–∞–ª–æ–≥–∞.
7. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –≤ internal_analysis:** 
    - –°–æ–ø–æ—Å—Ç–∞–≤—å –æ—Ç–≤–µ—Ç —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑, –Ω–∞–≤—ã–∫–æ–≤, –æ–ø—ã—Ç–∞. –í—ã–¥–µ–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ/–Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã.
    - –í—ã—è–≤–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è (—Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –≤ —Å—Ç–∞–∂–µ, –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è) –∏ "–∫—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏" (—à–∞–±–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã, —É–∫–ª–æ–Ω–µ–Ω–∏–µ).
    - –†–∞—Å—Å—á–∏—Ç–∞–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è: –û—Ü–µ–Ω–∏ –∫–∞–∂–¥—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –ø–æ –≤–µ—Å–∞–º, —Å—É–º–º–∏—Ä—É–π (0-100%).
    - **–í–ê–ñ–ù–û**: `internal_analysis` –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –°–¢–†–û–ö–û–ô, –∞ –Ω–µ —Å–ª–æ–≤–∞—Ä–µ–º. –°–µ—Ä–∏–∞–ª–∏–∑—É–π –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—Å—Ç.

**–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:**
{chat_history}

**–¢–≤–æ—è –∑–∞–¥–∞—á–∞:**
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–≤ –≤—Å—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤—ã—à–µ, —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π JSON-–æ–±—ä–µ–∫—Ç —Å –¥–≤—É–º—è –∫–ª—é—á–∞–º–∏:
1. `"assistant_response"`: –¢–≤–æ–π —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç—É (—Å—Ç—Ä–æ–∫–∞).
2. `"internal_analysis"`: –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (–°–¢–†–û–ö–ê), –≤–∫–ª—é—á–∞—é—â–∏–π:
   - –ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏/–æ–ø—ã—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: –∑–Ω–∞–Ω–∏–µ Python").
   - –ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏/–æ–ø—ã—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: –æ–ø—ã—Ç —Å SQL").
   - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ "–∫—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏" (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ö—Ä–∞—Å–Ω—ã–π —Ñ–ª–∞–≥: —É–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –≤–æ–ø—Ä–æ—Å–∞ –æ —Å—Ç–∞–∂–µ").
   - –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ü—Ä–æ—Ü–µ–Ω—Ç: 70% (technical_skills: 80%*0.5 + communication: 60%*0.3 + cases: 50%*0.2)").
   - –û—Ü–µ–Ω–∫—É —è—Å–Ω–æ—Å—Ç–∏, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞.

**–ü—Ä–∏–º–µ—Ä –≤—ã—Ö–æ–¥–∞ (JSON):**
{{
  "assistant_response": "–°–ø–∞—Å–∏–±–æ –∑–∞ —Ä–∞—Å—Å–∫–∞–∑! –£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –∫–∞–∫—É—é —Å–∏—Å—Ç–µ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –≤—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏?",
  "internal_analysis": "–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: –æ–ø—ã—Ç —Å React. –ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: –∑–Ω–∞–Ω–∏–µ Redux. –ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏: —É–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –¥–µ—Ç–∞–ª–µ–π. –ü—Ä–æ—Ü–µ–Ω—Ç: 70% (technical_skills: 80%*0.5 + communication: 60%*0.3 + cases: 50%*0.2). –û—Ç–≤–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω, –Ω–æ –Ω–µ–ø–æ–ª–Ω—ã–π."
}}

**–í–ê–ñ–ù–û:** –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –≥–¥–µ `internal_analysis` ‚Äî —Å—Ç—Ä–æ–∫–∞.

JSON_OUTPUT:
"""
)

final_report_prompt_template = PromptTemplate(
    input_variables=["dialogue_history", "requirements", "criteria_weights"],
    template="""–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ. –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–∞–ª–æ–≥–∞ ({dialogue_history}) –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–∏ ({requirements}) —Å –≤–µ—Å–∞–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ ({criteria_weights}) —Å–æ–∑–¥–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –≤ JSON-—Ñ–æ—Ä–º–∞—Ç–µ.

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**
- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥, —Å–æ–ø–æ—Å—Ç–∞–≤—å –æ—Ç–≤–µ—Ç—ã —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏, —É—á—Ç–∏ –≤–µ—Å–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤.
- –í–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç —Å –ø–æ–ª—è–º–∏:
  - "match_percentage": –ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (0-100%).
  - "competencies_breakdown": –¢–µ–∫—Å—Ç —Å —Å–∏–ª—å–Ω—ã–º–∏ —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: –ó–Ω–∞–Ω–∏–µ Python. –ü—Ä–æ–±–µ–ª—ã: –ù–µ—Ç –æ–ø—ã—Ç–∞ —Å SQL").
  - "recommendation": "–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø", "–æ—Ç–∫–∞–∑" –∏–ª–∏ "—Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ".
  - "notification_text": –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–í–∞—à —É—Ä–æ–≤–µ–Ω—å Python —Ö–æ—Ä–æ—à, –Ω–æ –Ω—É–∂–Ω–æ –ø–æ–¥—Ç—è–Ω—É—Ç—å SQL").

**–ü—Ä–∏–º–µ—Ä JSON:**
{{
  "match_percentage": 75,
  "competencies_breakdown": "–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: –û–ø—ã—Ç —Å Django, —Ö–æ—Ä–æ—à–∞—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è. –ü—Ä–æ–±–µ–ª—ã: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∑–Ω–∞–Ω–∏–π SQL.",
  "recommendation": "–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø",
  "notification_text": "–í–∞—à —É—Ä–æ–≤–µ–Ω—å Python —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º —É–≥–ª—É–±–∏—Ç—å –∑–Ω–∞–Ω–∏—è SQL."
}}

**–í–ê–ñ–ù–û:** –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON, –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞.

JSON_OUTPUT:
{dialogue_history}
"""
)

structure_prompt_template = PromptTemplate(
    input_variables=["user_text"],
    template="–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞: \"{user_text}\". –û—Ü–µ–Ω–∏ –µ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è, –Ω–∞–ª–∏—á–∏–µ —á–µ—Ç–∫–∏—Ö –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤, —Å–≤—è–∑–Ω–æ—Å—Ç—å –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è. –î–∞–π –∫—Ä–∞—Ç–∫—É—é –æ—Ü–µ–Ω–∫—É –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ soft skills –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º. –ü—Ä–∏–º–µ—Ä: '–û—Ç–≤–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω —Ö–æ—Ä–æ—à–æ, –∫–∞–Ω–¥–∏–¥–∞—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã.' –∏–ª–∏ '–ü–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –Ω–µ–º–Ω–æ–≥–æ —Å—É–º–±—É—Ä–Ω–æ–µ, –∫–∞–Ω–¥–∏–¥–∞—Ç –ø–µ—Ä–µ—Å–∫–∞–∫–∏–≤–∞–µ—Ç —Å —Ç–µ–º—ã –Ω–∞ —Ç–µ–º—É.'"
)

resume_analysis_prompt_template = PromptTemplate(
    input_variables=["resume_text", "vacancy", "requirements", "questions_list", "criteria_weights"],
    template="""–¢—ã ‚Äî AI-–∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä–µ–∑—é–º–µ, –ø—Ä–æ–≤–æ–¥—è—â–∏–π –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–∏ {vacancy}.

**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ:**
{resume_text}

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –≤–∞–∫–∞–Ω—Å–∏–∏:**
{requirements}

**–í–æ–ø—Ä–æ—Å—ã/—Ç–µ–º—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–π –∫–∞–∫ –æ—Ä–∏–µ–Ω—Ç–∏—Ä –¥–ª—è –Ω–∞–≤—ã–∫–æ–≤):**
{questions_list}

**–í–µ—Å–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ (—Å—É–º–º–∞ –≤–µ—Å–æ–≤ = 100):**
{criteria_weights}

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:**
- **–ò–∑–≤–ª–µ–∫–∏ –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ —Ä–µ–∑—é–º–µ:** –ù–∞–≤—ã–∫–∏ (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã), —Å—Ç–∞–∂ (–æ–±—â–∏–π –∏ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –ø–æ–∑–∏—Ü–∏—è–º), –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è/–ø—Ä–æ–µ–∫—Ç—ã, –ª–∏—á–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞.
- **–°–æ–ø–æ—Å—Ç–∞–≤—å —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏:** –í—ã–¥–µ–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏/–æ–ø—ã—Ç (—Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ —Ä–µ–∑—é–º–µ), –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ (—á—Ç–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç), –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç–∞–∂ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∑–∞—è–≤–ª–µ–Ω–Ω—ã–º –Ω–∞–≤—ã–∫–∞–º).
- **–ö—Ä–∞—Å–Ω—ã–µ —Ñ–ª–∞–≥–∏:** –í—ã—è–≤–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã (—É–∫–ª–æ–Ω—á–∏–≤—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, –ø—Ä–æ–±–µ–ª—ã –≤ –æ–ø—ã—Ç–µ, –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–∞—Ç, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π). –ü—Ä–∏–≤–µ–¥–∏ 2-3 –ø—Ä–∏–º–µ—Ä–∞.
- **–†–∞—Å—á—ë—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞:** –û—Ü–µ–Ω–∏ –∫–∞–∂–¥—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π (0-100%) –ø–æ –≤–µ—Å–∞–º, —Å—É–º–º–∏—Ä—É–π –æ–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç. –£—á–∏—Ç—ã–≤–∞–π —Å—Ç–∞–∂, –Ω–∞–≤—ã–∫–∏, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.
- **–†–∞–∑–±–∏–≤–∫–∞ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º:** –ü–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—à–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã (—á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç), —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã (–ø—Ä–æ–±–µ–ª—ã), —Å–æ–≤–µ—Ç—ã –ø–æ —É–ª—É—á—à–µ–Ω–∏—é (—á—Ç–æ –∏–∑—É—á–∏—Ç—å/–¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å).
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** "–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø" (–µ—Å–ª–∏ >70%), "–æ—Ç–∫–∞–∑" (–µ—Å–ª–∏ <50%), "—Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ" (–∏–Ω–∞—á–µ). –û–±–æ—Å–Ω—É–π.
- **–û—Ç–∑—ã–≤:** –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –º–æ—Ç–∏–≤–∏—Ä—É—é—â–∏–π, —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Å–æ–≤–µ—Ç–∞–º–∏).

**–í–µ—Ä–Ω–∏ —á–∏—Å—Ç—ã–π JSON –±–µ–∑ markdown-–æ–±—ë—Ä—Ç–æ–∫ (—Ç–∏–ø–∞ ```json
{{
  "match_percentage": 75,  // –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç (0-100)
  "key_skills_from_resume": ["Python", "C++", "JavaScript", "OpenCV", "PyTorch", "TensorFlow", "Git"],  // –ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏ –∏–∑ —Ä–µ–∑—é–º–µ (–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫)
  "experience_summary": "–°—Ç–∞–∂: 3 –≥–æ–¥–∞ –≤ backend. –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: –í—ã—Å—à–µ–µ (–ú–ì–£).",  // –ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä —Å—Ç–∞–∂–∞ –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
  "confirmed_skills": "–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: Python (3 –≥–æ–¥–∞ –æ–ø—ã—Ç–∞), Git. –ü—Ä–∏–º–µ—Ä—ã: –ü—Ä–æ–µ–∫—Ç—ã –Ω–∞ GitHub.",  // –ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
  "unconfirmed_skills": "–ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: REST API, –∞–Ω–≥–ª–∏–π—Å–∫–∏–π. –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Ä–µ–∑—é–º–µ.",  // –ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º
  "red_flags": ["–ü—Ä–æ–±–µ–ª –≤ –æ–ø—ã—Ç–µ 2020-2022 –≥–≥.", "–ù–µ—Ç quantifiable –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π (–º–µ—Ç—Ä–∏–∫)."],  // –ú–∞—Å—Å–∏–≤ –∫—Ä–∞—Å–Ω—ã—Ö —Ñ–ª–∞–≥–æ–≤
  "competencies_breakdown": "Technical skills (50%): 80% (—Ö–æ—Ä–æ—à–æ —Å Python, –Ω–æ —Å–ª–∞–± SQL). Communication (30%): 60%. Cases (20%): 50%. –°–æ–≤–µ—Ç—ã: –ò–∑—É—á–∏—Ç—å SQL, –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ä–µ–∑—é–º–µ.",  // –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ + —Å–æ–≤–µ—Ç—ã
  "recommendation": "–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —ç—Ç–∞–ø",  // –û–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
  "notification_text": "–í–∞—à –æ–ø—ã—Ç —Å Python –≤–ø–µ—á–∞—Ç–ª—è–µ—Ç, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º —É–≥–ª—É–±–∏—Ç—å –∑–Ω–∞–Ω–∏—è SQL –∏ –¥–æ–±–∞–≤–∏—Ç—å quantifiable –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ —Ä–µ–∑—é–º–µ. –ñ–¥—ë–º –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —ç—Ç–∞–ø–µ!"  // –û—Ç–∑—ã–≤ (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
}}

**–í–ê–ñ–ù–û:** –¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON. –ë—É–¥—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–º, –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–µ–∑—é–º–µ. –ï—Å–ª–∏ —Ä–µ–∑—é–º–µ –Ω–µ–ø–æ–ª–Ω–æ–µ, –æ—Ç–º–µ—Ç—å —ç—Ç–æ.

JSON_OUTPUT:
"""
)

# 3. ASR and Emotion Recognition Models
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π ASR (Whisper) –∏ Emotion Recognition (Hubert)...")
SAMPLE_RATE = 16000
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
asr_model = WhisperModel("large-v3", device=DEVICE, compute_type="float16")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/hubert-large-ls960-ft")
emotion_model = HubertForSequenceClassification.from_pretrained("xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned")
num2emotion = {0: 'neutral', 1: 'angry', 2: 'positive', 3: 'sad', 4: 'other'}
emotion_model.to(DEVICE)
print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

# =======================
# –§—É–Ω–∫—Ü–∏–∏-–ø–æ–º–æ—â–Ω–∏–∫–∏
# =======================
def split_text_into_chunks(text: str, sentences_per_chunk: int = 2) -> list[str]:
    """–†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ TTS."""
    sentences = re.split(r'(?<=[.!?‚Ä¶])\s+', text)
    chunks = []
    current_chunk = ""
    for i, sentence in enumerate(sentences):
        if not sentence: continue
        if current_chunk: current_chunk += " "
        current_chunk += sentence
        if (i + 1) % sentences_per_chunk == 0 or (i + 1) == len(sentences):
            chunks.append(current_chunk.strip())
            current_chunk = ""
    if current_chunk: chunks.append(current_chunk.strip())
    return [chunk for chunk in chunks if chunk]

def predict_emotion(audio_array: np.ndarray) -> str:
    """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —ç–º–æ—Ü–∏—é –ø–æ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã–º."""
    if audio_array.dtype != np.float32:
        audio_array = audio_array.astype(np.float32)
    inputs = feature_extractor(audio_array, sampling_rate=feature_extractor.sampling_rate, return_tensors="pt", padding=True, max_length=16000 * 10, truncation=True)
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    with torch.no_grad():
        logits = emotion_model(inputs['input_values']).logits
    predictions = torch.argmax(logits, dim=-1)
    return num2emotion[predictions.item()]

def analyze_structure(user_text: str) -> str:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM."""
    prompt = structure_prompt_template.format(user_text=user_text)
    return llm.invoke(prompt).content

def generate_and_analyze_response(interview_id: str, user_text: str, emotion: str, pauses_info: str, structure_analysis: str, vacancy_data: dict) -> LLMAnalysisResponse:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –∞–Ω–∞–ª–∏–∑, –≤–æ–∑–≤—Ä–∞—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç."""
    global chat_history
    current_history = chat_history.get(interview_id, [])
    current_history.append(("user", user_text))
    
    history_str = "\n".join([f"{'–ö–∞–Ω–¥–∏–¥–∞—Ç' if role == 'user' else '–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç'}: {content}" for role, content in current_history])
    
    prompt = prompt_template.format(
        chat_history=history_str, 
        emotion=emotion, 
        pauses_info=pauses_info,
        structure_analysis=structure_analysis, 
        vacancy=vacancy_data['title'],
        requirements=vacancy_data['requirements'],
        questions_list=vacancy_data['questions_list'],
        interview_script=vacancy_data['script'],
        criteria_weights=vacancy_data['criteria_weights']
    )
    
    try:
        response_content = llm.invoke(prompt).content.strip()
        print(f"LLM response in generate_and_analyze_response: {response_content}")  # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç LLM
        
        if not response_content:
            raise ValueError("LLM –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
        
        response_data = json.loads(response_content)
        
        # –ï—Å–ª–∏ internal_analysis –≤–µ—Ä–Ω—É–ª—Å—è –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
        if isinstance(response_data.get('internal_analysis'), dict):
            response_data['internal_analysis'] = json.dumps(response_data['internal_analysis'], ensure_ascii=False)
        
        analysis_response = LLMAnalysisResponse(**response_data)
    except (json.JSONDecodeError, TypeError, ValueError) as e:
        print(f"–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –≤ generate_and_analyze_response. –û—à–∏–±–∫–∞: {e}. –û—Ç–≤–µ—Ç LLM: {response_content}")
        analysis_response = LLMAnalysisResponse(
            assistant_response="–ü—Ä–æ—à—É –ø—Ä–æ—â–µ–Ω–∏—è, —É –º–µ–Ω—è –≤–æ–∑–Ω–∏–∫–ª–∞ –Ω–µ–±–æ–ª—å—à–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞. –ù–µ –º–æ–≥–ª–∏ –±—ã –≤—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å —Å–≤–æ–π –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç?",
            internal_analysis=f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: LLM –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –≤–∞–ª–∏–¥–Ω—ã–π JSON. –û—Ç–≤–µ—Ç –±—ã–ª: {response_content}"
        )
    
    current_history.append(("assistant", analysis_response.assistant_response))
    chat_history[interview_id] = current_history
    return analysis_response

def generate_final_report(report_path: Path, vacancy_data: dict):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç —Å –ø–æ–º–æ—â—å—é LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–∞–ª–æ–≥–∞."""
    with open(report_path, "r", encoding="utf-8") as f:
        report_data = json.load(f)
    
    dialogue_history = "\n".join([f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {turn.get('assistant_text', '')}\n–ö–∞–Ω–¥–∏–¥–∞—Ç: {turn.get('user_text', '')}\n–ê–Ω–∞–ª–∏–∑: {turn.get('internal_analysis', '')}" for turn in report_data["dialogue"]])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ–ª–µ–π
    requirements = vacancy_data.get('requirements', '–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω—ã.')
    criteria_weights = vacancy_data.get('criteria_weights', '{}')
    
    # –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ JSON
    prompt = final_report_prompt_template.format(
        dialogue_history=dialogue_history[:5000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
        requirements=requirements,
        criteria_weights=criteria_weights
    )
    
    try:
        response_content = llm.invoke(prompt).content.strip()
        print(f"LLM response: {response_content}")  # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        
        if not response_content:
            raise ValueError("LLM –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
        
        final_report = json.loads(response_content)
    except (json.JSONDecodeError, TypeError, ValueError) as e:
        print(f"–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞. –û—à–∏–±–∫–∞: {e}. –û—Ç–≤–µ—Ç LLM: {response_content}")
        final_report = {
            "match_percentage": 0,
            "competencies_breakdown": "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: LLM –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏–ª–∏ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.",
            "recommendation": "—Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ",
            "notification_text": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
        }
    
    report_data["final_report"] = final_report
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    return final_report

def extract_text_from_resume(file_content: bytes, file_extension: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ —Ä–µ–∑—é–º–µ (txt, pdf, doc, docx)."""
    try:
        if file_extension.lower() == 'txt':
            return file_content.decode('utf-8')
        elif file_extension.lower() in ['pdf', 'pdf.txt']:  # PDF
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pdfplumber –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                text = '\n'.join(page.extract_text() or '' for page in pdf.pages)
            return text if text else '–¢–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á—ë–Ω –∏–∑ PDF.'
        elif file_extension.lower() == 'docx':
            doc = Document(io.BytesIO(file_content))
            text = '\n'.join(para.text for para in doc.paragraphs)
            return text
        elif file_extension.lower() == 'doc':
            # –î–ª—è .doc –∏—Å–ø–æ–ª—å–∑—É–µ–º PyPDF2 –∫–∞–∫ fallback (–∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ –≤ DOCX –∑–∞—Ä–∞–Ω–µ–µ)
            # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: .doc —Å–ª–æ–∂–Ω–µ–µ, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ DOCX; –∑–¥–µ—Å—å –∑–∞–≥–ª—É—à–∫–∞
            return '–ü–æ–¥–¥–µ—Ä–∂–∫–∞ .doc –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º DOCX –∏–ª–∏ PDF.'
        else:
            return '–§–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.'
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
        return f'–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}'

def analyze_pauses(audio_array: np.ndarray, sample_rate: int = SAMPLE_RATE) -> str:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞—É–∑—ã (—Ç–∏—à–∏–Ω—É) –≤ –∞—É–¥–∏–æ –æ—Ç–≤–µ—Ç–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞."""
    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞—É–¥–∏–æ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (—É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [-1, 1])
        if np.max(np.abs(audio_array)) > 1.0:
            audio_array = audio_array / np.max(np.abs(audio_array))
        
        # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã —Ç–∏—à–∏–Ω—ã (–ø–∞—É–∑—ã) —Å –ø–æ—Ä–æ–≥–æ–º -20 dB (—Ç–∏—à–∏–Ω–∞ –Ω–∏–∂–µ —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è)
        intervals = librosa.effects.split(audio_array, top_db=20)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—É–∑ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
        total_pause_duration = sum((end - start) / sample_rate for start, end in intervals)
        num_pauses = len(intervals)
        avg_pause_duration = total_pause_duration / num_pauses if num_pauses > 0 else 0
        
        analysis = f"–û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–∞—É–∑: {total_pause_duration:.2f} —Å–µ–∫. ({num_pauses} –ø–∞—É–∑). –°—Ä–µ–¥–Ω—è—è –ø–∞—É–∑–∞: {avg_pause_duration:.2f} —Å–µ–∫."
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –¥–ª—è LLM (—Å–∏–≥–Ω–∞–ª—ã –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)
        if total_pause_duration > 5.0:
            analysis += " (–î–ª–∏–Ω–Ω—ã–µ –ø–∞—É–∑—ã ‚Äî –≤–æ–∑–º–æ–∂–Ω–∞—è –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏–ª–∏ —Ä–∞–∑–¥—É–º—å—è)."
        elif total_pause_duration < 1.0:
            analysis += " (–ö–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—É–∑—ã ‚Äî —É–≤–µ—Ä–µ–Ω–Ω–∞—è —Ä–µ—á—å)."
        
        return analysis
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—É–∑: {e}")
        return "–ê–Ω–∞–ª–∏–∑ –ø–∞—É–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞)."
    
# =======================
# API –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª–∏ –∏ –ê—Ä—Ö–∏–≤–∞
# =======================
@app.get("/admin")
async def get_admin_panel():
    try:
        with open("templates/admin.html", "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    except FileNotFoundError:
        return HTMLResponse("<h1>–û—à–∏–±–∫–∞: —Ñ–∞–π–ª templates/admin.html –Ω–µ –Ω–∞–π–¥–µ–Ω.</h1>", status_code=404)

@app.get("/archive")
async def get_archive_page():
    try:
        with open("templates/archive.html", "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="–§–∞–π–ª —à–∞–±–ª–æ–Ω–∞ –∞—Ä—Ö–∏–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    
@app.get("/analyze")
async def get_analyze_page():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ."""
    try:
        with open("templates/analyze.html", "r", encoding="utf-8") as f:
            return HTMLResponse(f.read())
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="–§–∞–π–ª templates/analyze.html –Ω–µ –Ω–∞–π–¥–µ–Ω.")

@app.get("/api/vacancies")
async def get_vacancies_list():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–π –∏ –Ω–∞–∑–≤–∞–Ω–∏–π –≤—Å–µ—Ö –≤–∞–∫–∞–Ω—Å–∏–π."""
    data = read_data()
    return [{"key": key, "title": value.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")} for key, value in data.items()]

@app.get("/api/vacancies/{key}")
async def get_vacancy_details(key: str):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏."""
    data = read_data()
    if key not in data:
        raise HTTPException(status_code=404, detail="–í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    return data[key]

@app.post("/api/vacancies", status_code=201)
async def create_vacancy(item: VacancyItem):
    """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é –≤–∞–∫–∞–Ω—Å–∏—é."""
    data = read_data()
    key = item.key
    if key in data:
        raise HTTPException(status_code=409, detail="–í–∞–∫–∞–Ω—Å–∏—è —Å —Ç–∞–∫–∏–º –∫–ª—é—á–æ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    data[key] = item.dict(exclude={'key'})
    write_data(data)
    return {"message": f"–í–∞–∫–∞–Ω—Å–∏—è '{key}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞"}

@app.put("/api/vacancies/{original_key}")
async def update_vacancy(original_key: str, item: VacancyItem):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤–∞–∫–∞–Ω—Å–∏—é."""
    data = read_data()
    if original_key not in data:
        raise HTTPException(status_code=404, detail="–†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–∞—è –≤–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    new_key = item.key
    if original_key != new_key:
        if new_key in data:
            raise HTTPException(status_code=409, detail=f"–ö–ª—é—á '{new_key}' —É–∂–µ –∑–∞–Ω—è—Ç –¥—Ä—É–≥–æ–π –≤–∞–∫–∞–Ω—Å–∏–µ–π")
        del data[original_key]
    
    data[new_key] = item.dict(exclude={'key'})
    write_data(data)
    return {"message": f"–í–∞–∫–∞–Ω—Å–∏—è '{new_key}' —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞"}

@app.delete("/api/vacancies/{key}")
async def delete_vacancy(key: str):
    """–£–¥–∞–ª—è–µ—Ç –≤–∞–∫–∞–Ω—Å–∏—é."""
    data = read_data()
    if key not in data:
        raise HTTPException(status_code=404, detail="–í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    del data[key]
    write_data(data)
    return {"message": f"–í–∞–∫–∞–Ω—Å–∏—è '{key}' —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞"}

@app.get("/api/reports")
async def get_reports_list():
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –ø–∞–ø–∫—É reports –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
    reports_meta = []
    for report_file in REPORTS_DIR.glob("*.json"):
        try:
            with open(report_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                reports_meta.append({
                    "id": data.get("interview_id"),
                    "title": data.get("vacancy_title"),
                    "date": data.get("date")
                })
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –æ—Ç—á–µ—Ç–∞ {report_file}: {e}")
    return reports_meta

@app.get("/api/reports/{interview_id}")
async def get_report_content(interview_id: str):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
    report_path = REPORTS_DIR / f"{interview_id}.json"
    if not report_path.exists():
        raise HTTPException(status_code=404, detail="–û—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    with open(report_path, "r", encoding="utf-8") as f:
        return json.load(f)

@app.post("/api/reports/{interview_id}/generate")
async def generate_report(interview_id: str):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –¥–ª—è —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è."""
    report_path = REPORTS_DIR / f"{interview_id}.json"
    if not report_path.exists():
        raise HTTPException(status_code=404, detail="–û—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    
    # –ù–∞—Ö–æ–¥–∏–º –≤–∞–∫–∞–Ω—Å–∏—é –ø–æ ID –æ—Ç—á—ë—Ç–∞
    with open(report_path, "r", encoding="utf-8") as f:
        report_data = json.load(f)
    vacancy_title = report_data.get("vacancy_title", "")
    interview_data = read_data()
    vacancy_key = [key for key, value in interview_data.items() if value["title"] == vacancy_title]
    vacancy_key = vacancy_key[0] if vacancy_key else "default"
    
    default_vacancy = {
        "title": "–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é",
        "requirements": "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω—ã.",
        "questions_list": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏.",
        "script": "–°–æ–æ–±—â–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ–± –æ—à–∏–±–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.",
        "criteria_weights": "{}"
    }
    vacancy_data = interview_data.get(vacancy_key, default_vacancy)
    
    print(f"Vacancy data for report generation: {vacancy_data}")  # –õ–æ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
    
    final_report = generate_final_report(report_path, vacancy_data)
    return final_report

class ResumeAnalysisRequest(BaseModel):
    vacancy_key: str  # –ö–ª—é—á –≤–∞–∫–∞–Ω—Å–∏–∏

@app.post("/api/resume/analyze")
async def analyze_resume(
    file: UploadFile = File(..., description="–§–∞–π–ª —Ä–µ–∑—é–º–µ (txt, pdf, doc, docx)"),
    vacancy_key: str = Form(..., description="–ö–ª—é—á –≤–∞–∫–∞–Ω—Å–∏–∏")
):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å –≤–∞–∫–∞–Ω—Å–∏–µ–π."""
    if not vacancy_key:
        raise HTTPException(status_code=400, detail="–£–∫–∞–∂–∏—Ç–µ –∫–ª—é—á –≤–∞–∫–∞–Ω—Å–∏–∏")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
    if not file.filename:
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
    
    file_extension = file.filename.split('.')[-1].lower()
    if file_extension not in ['txt', 'pdf', 'doc', 'docx']:
        raise HTTPException(status_code=400, detail="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: txt, pdf, doc, docx")
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    file_content = await file.read()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
    resume_text = extract_text_from_resume(file_content, file_extension)
    if not resume_text or "–û—à–∏–±–∫–∞" in resume_text:
        raise HTTPException(status_code=400, detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç: {resume_text}")
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
    interview_data = read_data()
    if vacancy_key not in interview_data:
        raise HTTPException(status_code=404, detail="–í–∞–∫–∞–Ω—Å–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    vacancy_data = interview_data[vacancy_key]
    questions_list = "\n".join(vacancy_data.get("questions", []))
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM
    prompt = resume_analysis_prompt_template.format(
        resume_text=resume_text,
        vacancy=vacancy_data["title"],
        requirements=vacancy_data.get("requirements", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω—ã."),
        questions_list=questions_list,
        criteria_weights=vacancy_data.get("criteria_weights", "{}")
    )
    
    try:
        response_content = llm.invoke(prompt).content.strip()
        print(f"LLM resume analysis response: {response_content}")
        
        if not response_content:
            raise ValueError("LLM –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç markdown-–æ–±—ë—Ä—Ç–æ–∫ (````json` –∏ ````)
        response_content = re.sub(r'^```json\s*|\s*```$', '', response_content.strip(), flags=re.MULTILINE)
        response_content = response_content.strip()  # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã/–ø–µ—Ä–µ–Ω–æ—Å—ã
        
        print(f"Cleaned response_content: {response_content}")  # –õ–æ–≥ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        
        analysis_result = json.loads(response_content)
        
        # –î–µ—Ñ–æ–ª—Ç—ã –¥–ª—è –Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π, –µ—Å–ª–∏ LLM –Ω–µ –≤–µ—Ä–Ω—É–ª–∞
        analysis_result.setdefault("key_skills_from_resume", [])
        analysis_result.setdefault("experience_summary", "–ù–µ —É–∫–∞–∑–∞–Ω–æ")
        analysis_result.setdefault("confirmed_skills", "–ù–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤")
        analysis_result.setdefault("unconfirmed_skills", "–í—Å–µ –Ω–∞–≤—ã–∫–∏ –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã")
        analysis_result.setdefault("red_flags", [])
        analysis_result.setdefault("competencies_breakdown", "–ù–µ—Ç —Ä–∞–∑–±–∏–≤–∫–∏")
        
    except (json.JSONDecodeError, ValueError) as e:
        print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—é–º–µ: {e}. –û—Ç–≤–µ—Ç LLM: {response_content}")
        analysis_result = {
            "match_percentage": 0,
            "key_skills_from_resume": [],
            "experience_summary": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
            "confirmed_skills": "",
            "unconfirmed_skills": "",
            "red_flags": ["–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞"],
            "competencies_breakdown": "–û—à–∏–±–∫–∞: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç LLM.",
            "recommendation": "—Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏–µ",
            "notification_text": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ä–µ–∑—é–º–µ."
        }
    
    return analysis_result

# =======================
# –û—Å–Ω–æ–≤–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# =======================
@app.get("/")
async def get():
    with open("templates/index.html", "r", encoding="utf-8") as f:
        return HTMLResponse(f.read())

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket, vacancy: str = "junior"):
    global chat_history
    await websocket.accept()
    
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
    interview_id = str(uuid.uuid4())
    chat_history[interview_id] = []
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏
    interview_data = read_data()
    vacancy_key = vacancy.lower().strip()
    
    if vacancy_key not in interview_data:
        vacancy_key = next(iter(interview_data)) if interview_data else "default"

    selected_vacancy_info = interview_data.get(vacancy_key, {
        "title": "–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é",
        "requirements": "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω—ã.",
        "questions": ["–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ –ø–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è."],
        "script": "–°–æ–æ–±—â–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ–± –æ—à–∏–±–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.",
        "criteria_weights": "{}"
    })
    
    vacancy_data_for_prompt = {
        "title": selected_vacancy_info["title"],
        "requirements": selected_vacancy_info.get("requirements", "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω—ã."),
        "questions_list": "\n".join(selected_vacancy_info["questions"]),
        "script": selected_vacancy_info["script"],
        "criteria_weights": selected_vacancy_info.get("criteria_weights", "{}")
    }

    # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –æ—Ç—á–µ—Ç–∞
    report_path = REPORTS_DIR / f"{interview_id}.json"
    report_data = {
        "interview_id": interview_id,
        "vacancy_title": vacancy_data_for_prompt["title"],
        "date": datetime.utcnow().isoformat(),
        "dialogue": []
    }
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    print(f"–ù–æ–≤–æ–µ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ. ID: {interview_id}. –û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_path}")

    # 4. –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
    initial_response = f"–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –î–∞–≤–∞–π—Ç–µ –Ω–∞—á–Ω–µ–º —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—é {selected_vacancy_info['title']}. –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ—Å—å, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏ —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –Ω–µ–º–Ω–æ–≥–æ –æ —Å–µ–±–µ."
    chat_history[interview_id].append(("assistant", initial_response))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –æ—Ç—á–µ—Ç
    report_data["dialogue"].append({
        "assistant_text": initial_response,
        "user_text": None,
        "emotion": None,
        "structure_analysis": None,
        "internal_analysis": "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞."
    })
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)

    initial_chunks = split_text_into_chunks(initial_response)
    for chunk_text in initial_chunks:
        await websocket.send_json({"type": "text", "sender": "assistant", "data": chunk_text})
        stream = tts_model.inference_stream(chunk_text, "ru", gpt_cond_latent, speaker_embedding, speed=1.0)
        for chunk_audio in stream:
            await websocket.send_bytes(chunk_audio.squeeze().cpu().numpy().tobytes())
    await websocket.send_json({"type": "audio_end"})

    # 5. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—â–µ–Ω–∏—è
    try:
        while True:
            message = await websocket.receive()

            if "text" in message:
                data = json.loads(message["text"])
                if data.get("type") == "end_interview":
                    farewell_message = "–°–ø–∞—Å–∏–±–æ –∑–∞ —É–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è! –ú—ã —Å–≤—è–∂–µ–º—Å—è —Å –≤–∞–º–∏ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è! –î–æ —Å–≤–∏–¥–∞–Ω–∏—è."
                    chat_history[interview_id].append(("assistant", farewell_message))
                    
                    farewell_chunks = split_text_into_chunks(farewell_message)
                    for chunk_text in farewell_chunks:
                        await websocket.send_json({"type": "text", "sender": "assistant", "data": chunk_text})
                        stream = tts_model.inference_stream(chunk_text, "ru", gpt_cond_latent, speaker_embedding, speed=1.0)
                        for chunk_audio in stream:
                            await websocket.send_bytes(chunk_audio.squeeze().cpu().numpy().tobytes())
                    
                    await websocket.send_json({"type": "audio_end"})
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –æ—Ç—á–µ—Ç
                    report_data["dialogue"].append({
                        "assistant_text": farewell_message,
                        "user_text": "[–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ]",
                        "emotion": None,
                        "structure_analysis": None,
                        "internal_analysis": "–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º."
                    })
                    with open(report_path, "w", encoding="utf-8") as f:
                        json.dump(report_data, f, ensure_ascii=False, indent=2)
                    await asyncio.sleep(0.1)
                    await websocket.close(code=1000)
                    break

            elif "bytes" in message:
                contents = message["bytes"]
                try:
                    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ
                    audio_segment = AudioSegment.from_file(io.BytesIO(contents))
                    wav_buffer = io.BytesIO()
                    audio_segment.export(wav_buffer, format="wav")
                    wav_buffer.seek(0)
                    audio_float, sr = sf.read(wav_buffer, dtype='float32')
                    if sr != SAMPLE_RATE:
                        audio_float = resampy.resample(audio_float, sr, SAMPLE_RATE, filter='kaiser_fast')

                    # ASR: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç
                    segments, _ = asr_model.transcribe(audio_float, language=None, beam_size=5)
                    user_text = "".join([s.text for s in segments]).strip()
                    
                    full_assistant_response = ""
                    if not user_text:
                        await websocket.send_json({"type": "text", "sender": "user", "data": "[–Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤–æ]"})
                        full_assistant_response = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –≤–∞—Å –Ω–µ —Ä–∞—Å—Å–ª—ã—à–∞–ª. –ù–µ –º–æ–≥–ª–∏ –±—ã –≤—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å?"
                        chat_history[interview_id].append(("assistant", full_assistant_response))
                        report_data["dialogue"].append({
                            "assistant_text": full_assistant_response,
                            "user_text": "[–Ω–µ—Ä–∞–∑–±–æ—Ä—á–∏–≤–æ]",
                            "emotion": None,
                            "structure_analysis": None,
                            "pauses_info": "N/A",  # –î–µ—Ñ–æ–ª—Ç
                            "internal_analysis": "–ö–∞–Ω–¥–∏–¥–∞—Ç –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª —Ä–∞–∑–±–æ—Ä—á–∏–≤—ã–π –æ—Ç–≤–µ—Ç."
                        })
                    else:
                        print(f"üë§ –ö–∞–Ω–¥–∏–¥–∞—Ç ({interview_id}): {user_text}")
                        await websocket.send_json({"type": "text", "sender": "user", "data": user_text})
                        
                        # –ê–Ω–∞–ª–∏–∑ —Ä–µ—á–∏: –ø–∞—É–∑—ã, —ç–º–æ—Ü–∏–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
                        pauses_info = analyze_pauses(audio_float, SAMPLE_RATE)  # –ù–æ–≤–æ–µ
                        emotion = predict_emotion(audio_float)
                        structure_analysis = analyze_structure(user_text)
                        analysis_response = generate_and_analyze_response(
                            interview_id, user_text, emotion, pauses_info, structure_analysis, vacancy_data_for_prompt
                        )
                        full_assistant_response = analysis_response.assistant_response
                        
                        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –æ—Ç—á—ë—Ç —Å –ø–∞—É–∑–∞–º–∏
                        report_data["dialogue"].append({
                            "assistant_text": full_assistant_response,
                            "user_text": user_text,
                            "emotion": emotion,
                            "structure_analysis": structure_analysis,
                            "pauses_info": pauses_info,  # –ù–æ–≤–æ–µ –ø–æ–ª–µ
                            "internal_analysis": analysis_response.internal_analysis
                        })
                    
                    with open(report_path, "w", encoding="utf-8") as f:
                        json.dump(report_data, f, ensure_ascii=False, indent=2)
                    
                    print(f"ü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç ({interview_id}): {full_assistant_response}")
                    
                    # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º (—Å—Ç—Ä–∏–º–∏–Ω–≥)
                    response_chunks = split_text_into_chunks(full_assistant_response)
                    if not response_chunks:
                        await websocket.send_json({"type": "audio_end"})
                        continue

                    for chunk_text in response_chunks:
                        print(f"  -> –û—Ç–ø—Ä–∞–≤–∫–∞ —á–∞–Ω–∫–∞: '{chunk_text}'")
                        await websocket.send_json({"type": "text", "sender": "assistant", "data": chunk_text})
                        stream = tts_model.inference_stream(chunk_text, "ru", gpt_cond_latent, speaker_embedding, speed=1.0)
                        for chunk_audio in stream:
                            await websocket.send_bytes(chunk_audio.squeeze().cpu().numpy().tobytes())
                    
                    await websocket.send_json({"type": "audio_end"})

                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: {e}")
                    await websocket.send_json({"type": "audio_end"})  # –†–∞–∑–±–ª–æ–∫–∏—Ä—É–µ–º –∫–Ω–æ–ø–∫—É –Ω–∞ –∫–ª–∏–µ–Ω—Ç–µ
                    continue

    except WebSocketDisconnect as e:
        print(f"–ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è (ID: {interview_id}). –ö–æ–¥: {e.code}")
        report_data["dialogue"].append({
            "assistant_text": None,
            "user_text": "[–ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è]",
            "emotion": None,
            "structure_analysis": None,
            "internal_analysis": f"–°–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ. –ö–æ–¥ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è: {e.code}"
        })
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
    finally:
        print(f"–°–µ—Å—Å–∏—è {interview_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")
        if interview_id in chat_history:
            del chat_history[interview_id]

# =======================
# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
# =======================
if __name__ == "__main__":
    print("‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ –∑–∞–ø—É—Å–∫—É.")
    print("‚úÖ –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ –∞–¥—Ä–µ—Å—É http://127.0.0.1:8000")
    print("‚úÖ –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∞–¥—Ä–µ—Å—É http://127.0.0.1:8000/admin")
    print("‚úÖ –ê—Ä—Ö–∏–≤ –æ—Ç—á–µ—Ç–æ–≤ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É http://127.0.0.1:8000/archive")
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)